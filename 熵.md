# 熵

$Entropy=\Sigma-p\log p$

负号：提供单调递减，概率越大的事发生了，提供的信息越小

log：提供$f(p_1p_2)=f(p_1)+f(p_2)$,两件事的熵加起来等于两件事一起发生的熵。

log外再乘以$p$: 该事件期望提供的信息熵

$\Sigma$:对整个系统期望贡献的信息量



![image-20231014192812323](C:/Users/1649019876/AppData/Roaming/Typora/typora-user-images/image-20231014192812323.png)

f取负对数的时候的期望

# 相对熵（KL散度）和交叉熵

![image-20231014192940342](C:/Users/1649019876/AppData/Roaming/Typora/typora-user-images/image-20231014192940342.png)

以P为基准Q的相对熵，就是度量P和Q的差别。

Q要达到和P一样的分布需要多少信息量。

该式可以分为两部分：

P的熵是确定不变的，P是基准

![image-20231014193246412](C:/Users/1649019876/AppData/Roaming/Typora/typora-user-images/image-20231014193246412.png)

这是交叉熵

![image-20231014193314332](C:/Users/1649019876/AppData/Roaming/Typora/typora-user-images/image-20231014193314332.png)

由吉布斯不等式，可以确保交叉熵比基准的熵小，也就是交叉熵一定大于等于0.要让Q的概率密度趋近于P的概率密度，那么交叉熵越小越好。

所以交叉熵越小越好。

H（P，Q）越小，Q越趋近于P



# 实际应用

![image-20231014194204708](C:/Users/1649019876/AppData/Roaming/Typora/typora-user-images/image-20231014194204708.png)

把交叉熵作为损失函数，$p_i$就是每个样本的ground truth label $x_i=0\ or\ 1$, 那么求和次数就是训练样本类数$n=2$（是和不是，二分类）。而$q_i$需要分类讨论，因为输出的$y_i$一般是一个连续量，也就是分对的概率。输出标签是x=1时，$q_i$是分对的概率$y_i$；x=0时，qi是分错的概率1-$y_i$。







# 相对熵和Wassertstein距离：都是度量两个分布的差异

**相对熵（KL散度）：**

1. **定义**：KL散度是用于度量两个概率分布之间的差异或信息损失的指标。它是一个非对称的度量，表示在用一个分布来近似另一个分布时产生的信息损失。

2. **计算方式**：KL散度通常通过对两个概率分布的相对概率取对数，然后取期望来计算。它的计算方式如下：

   KL(P||Q) = Σ (P(x) * log(P(x) / Q(x)))

   其中，P(x) 和 Q(x) 分别是两个概率分布在事件 x 上的概率。

3. **性质**：KL散度通常是非负的，即 KL(P||Q) >= 0，并且当且仅当 P 和 Q 完全相同时等于零。它不是一个距离度量，因为它不满足对称性和三角不等式。

4. **应用**：KL散度在信息论、统计学、机器学习中有广泛应用，例如在最大似然估计、变分推断和正则化中使用。

**Wasserstein距离：**

1. **定义**：Wasserstein距离，也称为Earth Mover's Distance（EMD），用于度量两个概率分布之间的距离或差异，它考虑了如何将一个分布转化为另一个的成本。与KL散度不同，它是对称的。
2. **计算方式**：Wasserstein距离通常通过找到从一个分布到另一个分布的最小“成本”传输方案来计算。这种计算需要解决线性规划问题或使用其他数值方法。
3. **性质**：Wasserstein距离是一个距离度量，满足对称性和三角不等式。它能更好地捕捉两个分布之间的结构和形状差异。
4. **应用**：Wasserstein距离在概率分布比较、图像处理、生成对抗网络（GANs）等领域有广泛的应用，尤其在生成模型的训练中。

总结：相对熵和Wasserstein距离是用于度量概率分布之间差异的不同工具，选择使用哪个取决于问题的性质和所需的性质。KL散度适用于信息论和概率模型中，而Wasserstein距离更适用于分布比较和生成模型中。
